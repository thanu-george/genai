## https://www.browserstack.com/guide/python-selenium-to-run-web-automation-test
## https://dev.to/stephenc222/how-to-use-milvus-to-store-and-query-vector-embeddings-5hhl
## https://poloclub.github.io/transformer-explainer/
## https://dugas.ch/artificial_curiosity/GPT_architecture.html



## ﻿SELENIUM
Open source framework for automating web browsers
* Testing web applications
* Performing automation tasks
* Scraping data from website


## CHROMEDRIVER : separate executable that selenium webdriver uses to launch Google chrome
* Used by selenium to communicate test scripts with chrome
WEBDRIVER:  collection of APIs that are used for automating the testing of web applications


## Input URL:
`https://www.example.com/site/page/?search=test#section1`
Parsed parts:
        •        scheme: `https`
        •        netloc (domain): `www.example.com`
        •        path: `/site/page/`
        •        query: `search=test`
        •        fragment: `section1`
Normalization steps:
        •        Remove fragment → ignore `#section1`
        •        Remove trailing slash from path `/site/page/` → `/site/page`
        •        Rebuild cleaned URL: `https://www.example.com/site/page/?search=test`



## What should be cleaned in a markdown file before chunking?
Before splitting a document into chunks (for tasks like retrieval-augmented generation), cleaning helps ensure chunks are meaningful and concise:
        •        Remove navigation, repeating headers/footers, and boilerplate that appear in every page.
        •        Delete ads, social media links, and unrelated widgets.
        •        Strip out metadata, tracking codes, and HTML artifacts leftover after conversion.
        •        Condense multiple consecutive blank lines into single newlines.
        •        Remove redundant or irrelevant legal disclaimers or copyright notices.
        •        Fix encoding or Unicode artifacts and normalize whitespace.
        •        Optionally remove side comments or footnotes if they do not add context.
        •        Fix isolated punctuation or incomplete sentences that can confuse chunking/sentence segmentation.


## Why are your scores low?
1. Embedding model mismatch
   * You’re using all-mpnet-base-v2 (a general-purpose embedding model).
   * It’s good, but not optimized for semantic search in QA pipelines.
   * Some chunks may be long or contain multiple concepts → weaker matches.
2. Chunking strategy
   * If chunks are too large (1000 tokens), they may include unrelated text, diluting similarity.
   * If too small, context may get lost.
3. Query formulation
   * "What is a term sheet?" is phrased as a question.
   * Embedding models often retrieve better if you rephrase queries to be more keyword-focused, e.g. "definition of term sheet".


## How to improve retrieval
1. Switch to a better embedding model
   * Try text-embedding-3-large (OpenAI, if allowed).
   * For open-source: bge-large-en or gte-large (HuggingFace).
   * These often outperform mpnet in retrieval tasks.
2. Improve chunking
   * Try smaller chunks (chunk_size=500, chunk_overlap=100).
   * This makes chunks tighter and more focused.
3. Query expansion / rewriting
   * Use the LLM itself to rephrase the query into multiple semantic variations and search with all.
   * Example: "term sheet definition", "explain startup term sheet", etc.
4. Hybrid search (semantic + keyword/BM25)
   * Milvus supports hybrid search — you can combine embeddings with keyword matches for stronger recall.

